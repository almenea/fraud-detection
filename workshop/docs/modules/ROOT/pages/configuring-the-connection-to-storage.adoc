:_module-type: PROCEDURE

[id='configuring-the-connection-to-storage']
= Configuring the connection to storage

[role="_abstract"]
In node 2, the notebook uploads the model to the S3 storage bucket. You must set the S3 storage bucket keys by using the secret created by the `My Storage` connection that you set up in xref:storing-data-with-connections.adoc[Storing data with connections].

You can use this secret in your pipeline nodes without having to save the information in your pipeline code. This is important, for example, if you want to save your pipelines - without any secret keys - to source control.

The name of the secret is `aws-connection-my-storage`. 

[NOTE] 
====
If you named your connection something other than `My Storage`, you can obtain the secret name in the {productname-short} dashboard by hovering over the help (?) icon in the *Connections* tab. 

image::pipelines/dsp-dc-secret-name.png[My Storage Secret Name, 400]
====

The `aws-connection-my-storage` secret includes the following fields:

* `AWS_ACCESS_KEY_ID`
* `AWS_DEFAULT_REGION`
* `AWS_S3_BUCKET`
* `AWS_S3_ENDPOINT`
* `AWS_SECRET_ACCESS_KEY`

You must set the secret name and key for each of these fields.

.Prerequisites

*  You created the `My Storage` connection, as described in xref:storing-data-with-connections.adoc[Storing data with connections].

* You set the `models/fraud/1/model.onnx` file as the output file for both nodes in your pipeline, as described in
xref:creating-and-storing-the-onnx-output-file.adoc[Creating and storing the ONNX-formatted output file].

.Procedure

. Remove any pre-filled environment variables.

.. Select node 2, and then select the *Node Properties* tab.
+
Under *Additional Properties*, note that some environment variables have been pre-filled. The pipeline editor inferred that you need them from the notebook code.  
+
Because you do not want to save the value in your pipelines, remove all of these environment variables.

.. Click *Remove* for each of the pre-filled environment variables.
+
image::pipelines/wb-pipeline-node-remove-env-var.png[Remove Env Var]

. Add the S3 bucket and keys by using the Kubernetes secret.

.. Under *Kubernetes Secrets*, click *Add*.
+
image::pipelines/wb-pipeline-add-kube-secret.png[Add Kubernetes Secret]

.. Enter the following values and then click *Add*.
+
* *Environment Variable*: `AWS_ACCESS_KEY_ID`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_ACCESS_KEY_ID`
+
image::pipelines/wb-pipeline-kube-secret-form.png[Secret Form, 400]

. Repeat Step 2 for each of the following Kubernetes secrets:

* *Environment Variable*: `AWS_SECRET_ACCESS_KEY`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_SECRET_ACCESS_KEY`

* *Environment Variable*: `AWS_S3_ENDPOINT`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_S3_ENDPOINT`

* *Environment Variable*: `AWS_DEFAULT_REGION`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_DEFAULT_REGION`

* *Environment Variable*: `AWS_S3_BUCKET`
** *Secret Name*: `aws-connection-my-storage`
** *Secret Key*: `AWS_S3_BUCKET`

. Select *File* -> *Save Pipeline As* to save and rename the pipeline. For example, rename it to `My Train Save.pipeline`.

.Verification

* You set the S3 storage bucket keys by using the secret created by the `My Storage` connection.

.Next step

xref:running-your-pipeline.adoc[Running your pipeline]
